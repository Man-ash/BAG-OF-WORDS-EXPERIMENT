{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "from scipy.stats import multivariate_normal \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100, 147)\n"
     ]
    }
   ],
   "source": [
    "root = os.getcwd()\n",
    "train_folder = os.path.join(root,\"E:\\Eck Module-3 Unsupervised Learning, Genratives Models,Pattern Discovery\\IITG_PG AI&ML-03-06-2020 -Clustering using Bag of Words approach\\Train_images\")\n",
    "train_files = os.listdir(train_folder)\n",
    "data_arr = []\n",
    "for i in range(len(train_files)):\n",
    "    file = os.path.join(train_folder,train_files[i])\n",
    "    image_array = mpimg.imread(file)\n",
    "    image_patches = extract_patches_2d(image_array, (7, 7), max_patches = 100)\n",
    "    for j in range(len(image_patches)):\n",
    "        patch_vec = np.ravel(image_patches[j])\n",
    "        data_arr.append(patch_vec)\n",
    "data_arr = np.matrix(data_arr)\n",
    "print(data_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 0. 0. ... 2. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "K = 3 ## K is the number of clusters that we want to create \n",
    "label_arr = np.zeros(data_arr.shape[0])\n",
    "for i in range(len(label_arr)):\n",
    "    label_arr[i] = np.random.choice(K)\n",
    "print(label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(vec1,vec2):\n",
    "    \n",
    "    s1 = 0\n",
    "    \n",
    "    vec1 = np.ravel(vec1)\n",
    "    vec1 = vec1/np.linalg.norm(vec1)\n",
    "\n",
    "    vec2 = np.ravel(vec2)\n",
    "    vec2 = vec2/np.linalg.norm(vec2)\n",
    "    \n",
    "    L = len(vec1)\n",
    "    \n",
    "    for l in range(L):\n",
    "        diff = vec2[l]*vec1[l]\n",
    "        s1 = s1 + diff\n",
    "    sim = s1\n",
    "    \n",
    "    return(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mean_cov(K,data_arr,label_arr):\n",
    "    mean_ls = [] ## List containing mean values of the clusters\n",
    "    cov_ls = []\n",
    "    size_ls = []\n",
    "    cluster_ls = [[] for k in range(K)] ## Create list of empty lists to store data belonging to a certain cluster\n",
    "    \n",
    "    for i in range(len(label_arr)):\n",
    "        for k in range(K):\n",
    "            if label_arr[i] == k:  ## if the label of the data at ith row is 'k'\n",
    "                norm_data = np.ravel(data_arr[i,:])/np.linalg.norm(np.ravel(data_arr[i,:]))\n",
    "                cluster_ls[k].append(norm_data) ## Fill the kth empty list with this data value                \n",
    "    \n",
    "    for k in range(K): \n",
    "        cluster_mat = np.matrix(cluster_ls[k])\n",
    "        pointNum = cluster_mat.shape[0]\n",
    "        cov_k = np.cov(cluster_mat.T)\n",
    "        mean_k = np.mean(cluster_mat,axis=0)\n",
    "        mean_k = np.ravel(mean_k)/np.linalg.norm(np.ravel(mean_k))\n",
    "        mean_ls.append(mean_k)\n",
    "        cov_ls.append(cov_k)\n",
    "        size_ls.append(pointNum)\n",
    "    return(mean_ls,cov_ls,size_ls)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_update(prev_mean,data_arr,label_arr):\n",
    "    for i in range(data_arr.shape[0]):\n",
    "        dist_ls = [] \n",
    "        for k in range(len(prev_mean)):\n",
    "            dist = similarity(data_arr[i,:],prev_mean[k]) ## Calculate the similarity of the ith datapoint with the kth mean\n",
    "            dist_ls.append(dist) ## Put the distance values in a list\n",
    "        dist_arr = np.array(dist_ls) ## Convert it to a NumPy array\n",
    "        new_label = np.argmax(dist_arr) ##The new_label of the point is the one which is closest to the ith datapoint,i.e., it has maximum similarity\n",
    "        label_arr[i] = new_label ## Set the new label\n",
    "    return(label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_from_label(K,prev_mean,prev_cov,prev_size,data_arr,label_arr):\n",
    "    cluster_ls = [[] for k in range(K)]  ## Create list of empty lists to store data belonging to a certain cluster\n",
    "    \n",
    "    for i in range(data_arr.shape[0]):\n",
    "        for k in range(K):\n",
    "            if label_arr[i] == k: ## if the label of the pixel at location [i,j] is 'k'\n",
    "                norm_data = np.ravel(data_arr[i,:])/np.linalg.norm(np.ravel(data_arr[i,:]))\n",
    "                cluster_ls[k].append(norm_data) ## Fill the kth empty list with this pixel value\n",
    "                    \n",
    "    for k in range(K):\n",
    "        if len(cluster_ls[k]) !=0:  ## Only update the means of those clusters which has received at least one new point, else retain the old mean value\n",
    "            cluster_mat = np.matrix(cluster_ls[k])\n",
    "            pointNum = cluster_mat.shape[0]\n",
    "            mean_k = np.mean(cluster_mat,axis=0)\n",
    "            cov_k = np.cov(cluster_mat.T)\n",
    "            mean_k = np.ravel(mean_k)/np.linalg.norm(np.ravel(mean_k))\n",
    "            prev_mean[k] = mean_k\n",
    "            prev_cov[k] = cov_k\n",
    "            prev_size[k] = pointNum\n",
    "    new_mean = prev_mean\n",
    "    new_cov = prev_cov\n",
    "    new_size = prev_size\n",
    "    return(new_mean,new_cov,new_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SphericalKMeans(data_arr,label_arr,K,maxIter):\n",
    "    mean_old,cov_old,size_old = init_mean_cov(K,data_arr,label_arr)\n",
    "    for t in range(maxIter):\n",
    "        new_label_arr = label_update(mean_old,data_arr,label_arr)\n",
    "        mean_new,cov_new,size_new = mean_from_label(K,mean_old,cov_old,size_old,data_arr,new_label_arr)\n",
    "        label_arr = new_label_arr ## Update the label array\n",
    "        mean_old = mean_new ## Update the mean values\n",
    "        cov_old = cov_new\n",
    "        size_old = size_new\n",
    "        print(\"Iteration {} is complete during training!!\".format(t+1))\n",
    "    return(mean_new,cov_new,size_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 is complete during training!!\n",
      "Iteration 2 is complete during training!!\n",
      "Iteration 3 is complete during training!!\n",
      "Iteration 4 is complete during training!!\n",
      "Iteration 5 is complete during training!!\n",
      "Iteration 6 is complete during training!!\n",
      "Iteration 7 is complete during training!!\n",
      "Iteration 8 is complete during training!!\n",
      "Iteration 9 is complete during training!!\n",
      "Iteration 10 is complete during training!!\n",
      "Iteration 11 is complete during training!!\n",
      "Iteration 12 is complete during training!!\n",
      "Iteration 13 is complete during training!!\n",
      "Iteration 14 is complete during training!!\n",
      "Iteration 15 is complete during training!!\n",
      "Iteration 16 is complete during training!!\n",
      "Iteration 17 is complete during training!!\n",
      "Iteration 18 is complete during training!!\n",
      "Iteration 19 is complete during training!!\n",
      "Iteration 20 is complete during training!!\n"
     ]
    }
   ],
   "source": [
    "mean_new,cov_new,size_new = SphericalKMeans(data_arr,label_arr,K,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33333333 0.3452381  0.32142857]\n"
     ]
    }
   ],
   "source": [
    "prior_ls = size_new/np.sum(size_new)\n",
    "print(prior_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testImage(img_file,mean_new,cov_new,prior_ls):\n",
    "    img_arr = mpimg.imread(img_file)\n",
    "    img_patches = extract_patches_2d(img_arr, (7, 7), max_patches = 50)\n",
    "    test_arr = []\n",
    "    for i in range(len(img_patches)):\n",
    "        patch_vec = np.ravel(img_patches[i])\n",
    "        test_arr.append(patch_vec)\n",
    "    test_arr = np.matrix(test_arr)\n",
    "    print(test_arr.shape)\n",
    "    for j in range(test_arr.shape[0]):\n",
    "        feat_vec = []\n",
    "        for k in range(len(size_new)):\n",
    "            var = multivariate_normal(mean = mean_new[k],cov = cov_new[k])\n",
    "            test1 = np.ravel(test_arr[j,:])\n",
    "            test_sample = test1/np.linalg.norm(test1)\n",
    "            lkl = var.pdf(test_sample)\n",
    "            post = lkl*prior_ls[k]\n",
    "            feat_vec.append(post)\n",
    "        print(feat_vec/sum(feat_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_img2.jpg\n",
      "(50, 147)\n",
      "[1.00000000e+000 0.00000000e+000 3.39809113e-255]\n",
      "[2.62202346e-021 1.33910207e-109 1.00000000e+000]\n",
      "[6.28691000e-047 6.59026611e-110 1.00000000e+000]\n",
      "[ 0. nan  0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_multivariate.py:757: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.logpdf(x))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e+000 6.11034737e-192 1.17122492e-178]\n",
      "[ 0.  0. nan]\n",
      "[2.05130395e-17 0.00000000e+00 1.00000000e+00]\n",
      "[1.00000000e+000 0.00000000e+000 3.77573564e-221]\n",
      "[9.99813275e-01 0.00000000e+00 1.86724767e-04]\n",
      "[ 0. nan  0.]\n",
      "[1.00000000e+000 0.00000000e+000 7.02874666e-198]\n",
      "[1.0000000e+00 0.0000000e+00 2.3258161e-57]\n",
      "[1.00000000e+00 0.00000000e+00 2.95533306e-85]\n",
      "[9.99999995e-001 5.29504270e-009 4.03665818e-103]\n",
      "[ 0. nan nan]\n",
      "[1.00000000e+000 0.00000000e+000 1.09728989e-309]\n",
      "[1.00000000e+000 0.00000000e+000 1.20738723e-214]\n",
      "[1. 0. 0.]\n",
      "[ 0. nan  0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[7.94902552e-007 9.99999205e-001 2.78494417e-195]\n",
      "[7.22157096e-24 0.00000000e+00 1.00000000e+00]\n",
      "[ 0.  0. nan]\n",
      "[9.99999945e-01 0.00000000e+00 5.54486167e-08]\n",
      "[ 0. nan  0.]\n",
      "[1. 0. 0.]\n",
      "[3.2881165e-33 0.0000000e+00 1.0000000e+00]\n",
      "[2.44803867e-43 0.00000000e+00 1.00000000e+00]\n",
      "[1.00000000e+000 0.00000000e+000 9.68659741e-220]\n",
      "[4.37383609e-10 0.00000000e+00 1.00000000e+00]\n",
      "[ 0. nan nan]\n",
      "[ 0.  0. nan]\n",
      "[1.00000000e+00 0.00000000e+00 4.29319958e-17]\n",
      "[ 0.  0. nan]\n",
      "[1.00000000e+00 0.00000000e+00 2.29244892e-29]\n",
      "[1.00000000e+000 0.00000000e+000 4.71959868e-136]\n",
      "[1.00000000e+000 0.00000000e+000 1.92620234e-230]\n",
      "[ 0. nan  0.]\n",
      "[1.85610982e-45 0.00000000e+00 1.00000000e+00]\n",
      "[1. 0. 0.]\n",
      "[9.99997230e-01 0.00000000e+00 2.77022308e-06]\n",
      "[1.00000000e+000 0.00000000e+000 2.28916259e-182]\n",
      "[1.000000e+000 0.000000e+000 2.588417e-160]\n",
      "[1.00000000e+000 0.00000000e+000 5.25484255e-163]\n",
      "[1.09103064e-08 0.00000000e+00 9.99999989e-01]\n",
      "[1.00000000e+00 0.00000000e+00 6.93549661e-36]\n",
      "[1.00000000e+000 8.68767244e-014 5.89292477e-133]\n",
      "[0.40855507 0.         0.59144493]\n",
      "[ 0. nan  0.]\n"
     ]
    }
   ],
   "source": [
    "test_folder = os.path.join(root,\"E:\\Eck Module-3 Unsupervised Learning, Genratives Models,Pattern Discovery\\IITG_PG AI&ML-03-06-2020 -Clustering using Bag of Words approach\\Test_images\")\n",
    "img_files = os.listdir(test_folder)\n",
    "fileName = random.choice(img_files)\n",
    "print(fileName)\n",
    "filePath = os.path.join(test_folder,fileName)\n",
    "testImage(filePath,mean_new,cov_new,prior_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MANASH PRATIM KAKATI\n",
    "##PG CERTIFICATION AI & ML\n",
    "## E&ICT ACADAMY, IIT GUWAHATI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
